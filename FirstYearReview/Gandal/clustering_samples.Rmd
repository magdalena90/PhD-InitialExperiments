---
title: 'Sample Clustering'
output:
  html_document:
    code_folding: 'hide'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=TRUE, include=FALSE}
#setwd('/afs/inf.ed.ac.uk/user/s17/s1725186/Documents/PhD-InitialExperiments/FirstYearReview/Gandal')

library(tidyverse) ; library(reshape2) ; library(glue) ; library(plotly) ; library(plotlyutils)
library(RColorBrewer) ; library(viridis) ; require(gridExtra) ; library(GGally)
library(ConsensusClusterPlus)
library(JADE) ; library(MineICA) ; library(moments) ; library(fdrtool)
library(ClusterR)
library(WGCNA)
library(pdfCluster) ; library(gplots) ; library(dendextend)
```
<br><br>

Load preprocessed dataset (preprocessing code in data_preprocessing.Rmd)
```{r, echo=TRUE, include=FALSE, warning=FALSE, message=FALSE}
load('./../Data/Gandal/preprocessed_data.RData')
datExpr = datExpr %>% data.frame
DE_info = DE_info %>% data.frame
```

```{r}
glue('Number of genes: ', nrow(datExpr), '\n',
     'Number of samples: ', ncol(datExpr), ' (', sum(datMeta$Diagnosis_=='ASD'), ' ASD, ',
     sum(datMeta$Diagnosis_!='ASD'), ' controls)')
```

#### Filtering DE Genes

Significance criteria: adjusted p-value<0.05
```{r}
datExpr = datExpr %>% filter(DE_info$padj<0.05 & !is.na(DE_info$padj))
print(paste0(nrow(datExpr), ' genes left.'))
```

#### Dimensionality reduction using PCA

Since there are more genes than samples, we can perform PCA and reduce the dimension from 30K to 80 without losing any information and use this for methods that take too long with the whole dataset
```{r}
datExpr_t = datExpr %>% t
pca = datExpr_t %>% prcomp
datExpr_redDim = pca$x %>% data.frame
```

## Clustering Methods

```{r}
clusterings = list()
```
<br><br>

### K-means clustering

Chose k=3 as best number of clusters
```{r}
set.seed(123)
wss = sapply(1:10, function(k) kmeans(datExpr_t, k, nstart=25)$tot.withinss)
plot(wss, type='b', main='K-Means Clustering')
best_k = 3
abline(v = best_k, col='blue')

datExpr_k_means = kmeans(datExpr_t, best_k, nstart=25)
clusterings[['KMeans']] = datExpr_k_means$cluster
```
<br><br>

### Hierarchical Clustering

Chose k=7 as best number of clusters.
```{r, fig.width=10}
h_clusts = datExpr_t %>% dist %>% hclust %>% as.dendrogram
h_clusts %>% plot
abline(h=25, col='blue')
best_k = 7
```

Clusters seem to be able to separate ASD and control samples pretty well and there are no noticeable patterns regarding sex, age or brain region in any cluster.

Colors:

- Diagnosis: Blue=control, Green=ASD

- Sex: Pink=Female, Blue=Male

- Brain region: Pink=Frontal, Green=Temporal, Blue=Parietal, Purple=Occipital

- Age: Purple=youngest, Yellow=oldest

```{r, fig.width=10}
clusterings[['HC']] = cutree(h_clusts, best_k)

create_viridis_dict = function(){
  min_age = datMeta$Age %>% min
  max_age = datMeta$Age %>% max
  viridis_age_cols = viridis(max_age - min_age + 1)
  names(viridis_age_cols) = seq(min_age, max_age)
  
  return(viridis_age_cols)
}
viridis_age_cols = create_viridis_dict()

dend_meta = datMeta[match(labels(h_clusts), rownames(datMeta)),] %>% 
            mutate('Diagnosis' = ifelse(Diagnosis_=='CTL','#008080','#86b300'), # Blue control, Green ASD
                   'Sex' = ifelse(Sex=='F','#ff6666','#008ae6'),                # Pink Female, Blue Male
                   'Region' = case_when(Brain_lobe=='Frontal'~'#F8766D',        # ggplot defaults for 4 colours
                                        Brain_lobe=='Temporal'~'#7CAE00',
                                        Brain_lobe=='Parietal'~'#00BFC4',
                                        Brain_lobe=='Occipital'~'#C77CFF'),
                   'Age' = viridis_age_cols[as.character(Age)]) %>%            # Purple: young, Yellow: old
            dplyr::select(Age, Region, Sex, Diagnosis)
h_clusts %>% set('labels', rep('', nrow(datMeta))) %>% set('branches_k_color', k=best_k) %>% plot(ylim=c(12,40))
colored_bars(colors=dend_meta)#, y_scale=10) # for the PDF
```
<br><br>

### Consensus Clustering

Chose the best clustering to be with k=8, which is basically two big clusters and some outliers
```{r echo=FALSE, message=FALSE}
cc_output = datExpr_t %>% as.matrix %>% t %>% 
  ConsensusClusterPlus(maxK=10, reps=50, seed=123, title='./../Data/Gandal/consensusClustering/samples_DE_genes', plot='png')
best_k = 8
clusterings[['CC']] = cc_output[[best_k]]$consensusClass
```

```{r, echo=FALSE, out.width = '50%'}
knitr::include_graphics('./../Data/Gandal/consensusClustering/samples_DE_genes/consensus008.png')
```

*The rest of the output plots can be found in the Data/Gandal/consensusClustering/samples_DE_genes folder

<br><br>

### Independent Component Analysis

Following [this paper's](www.journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1002367) guidelines:

0. Run PCA and keep enough components to explain 60% of the variance (The 2 first components explain over 60% of the variance, so decided to keep the first 37 to accumulate 90% of the variance)

1. Run ICA with that same number of nbComp as principal components kept to then filter them

2. Select components with kurtosis > 3

3. Assign obs to genes with FDR<0.01 using the fdrtool package

Note: Using the PCA reduced matrix because the algorithm didn't converge with the original dataset

```{r echo=FALSE, message=FALSE, warning=FALSE}
keep=30
ICA_output = datExpr_redDim[,1:keep] %>% runICA(nbComp=keep, method='JADE')
signals_w_kurtosis = ICA_output$S %>% data.frame %>% dplyr::select(colnames(ICA_output$S)[apply(ICA_output$S, 2, kurtosis)>3])
ICA_clusters = apply(signals_w_kurtosis, 2, function(x) fdrtool(x, plot=F, verbose=F)$qval<0.01) %>% data.frame
ICA_clusters = ICA_clusters[colSums(ICA_clusters)>1]

clusterings[['ICA']] = rep(NA, nrow(ICA_clusters))
ICA_clusters_names = colnames(ICA_clusters)
for(c in ICA_clusters_names) clusterings[['ICA']][ICA_clusters[,c]] = c
```
<br>

It's not supposed to be a good method for clustering samples because ICA does not perform well with small samples (see [Figure 4](https://www.nature.com/articles/s41467-018-03424-4/figures/4) of [this paper](https://www.nature.com/articles/s41467-018-03424-4))

<!-- 2. Warnings: (Warning in fdrtool(x, plot = F): There may be too few input test statistics for reliable FDR calculations!) -->

Still, it leaves only 7 samples without a cluster
```{r, fig.width=10}
ICA_clusters %>% rowSums %>% table

# ICA_clusters %>% mutate(cl_sum=rowSums(.)) %>% as.matrix %>% melt %>% ggplot(aes(Var2,Var1)) + 
#   geom_tile(aes(fill=value)) + xlab('Clusters') + ylab('Samples') + 
#   theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + coord_flip()
```
<br><br>

### WGCNA

This method doesn't work well:

Using power=5 as it is the first power that exceed the 0.85 threshold and the reduced version of the dataset because the original one never reaches the $R^2$ threshold
```{r, warning=FALSE}
best_power = datExpr_redDim %>% t %>% pickSoftThreshold(powerVector = 1:30)
```

Running WGCNA with power=5
```{r}
# Best power
network = datExpr_redDim %>% t %>% blockwiseModules(power=best_power$powerEstimate, numericLabels=TRUE)
```

Cluster distribution
```{r}
table(network$colors)
clusterings[['WGCNA']] = network$colors
names(clusterings[['WGCNA']]) = rownames(datExpr_redDim)
```

Using the original expression dataset, the $R^2$ threshold is never achieved, getting closest at power 1, but still doesn't manage to find any clusters within the data
```{r, warning=FALSE}
best_power = datExpr %>% pickSoftThreshold(powerVector = 1:30)
```
<br><br>

### Gaussian Mixture Models with hard thresholding

Points don't seem to follow a Gaussian distribution no matter the number of clusters, chose 4 groups following the best k from K-means because the methods are similar
```{r}
n_clust = datExpr_redDim %>% Optimal_Clusters_GMM(max_clusters=50, criterion='BIC', plot_data=FALSE)
plot(n_clust, type='l', main='Bayesian Information Criterion to choose number of clusters')
best_k = 3 # copying k-means best_k
gmm = datExpr_redDim %>% GMM(best_k)
clusterings[['GMM']] = gmm$Log_likelihood %>% apply(1, which.max)
```

Plot of clusters with their centroids in gray
```{r}
gmm_points = rbind(datExpr_redDim, setNames(data.frame(gmm$centroids), names(datExpr_redDim)))
gmm_labels = c(clusterings[['GMM']], rep(NA, best_k)) %>% as.factor
ggplotly(gmm_points %>% ggplot(aes(x=PC1, y=PC2, color=gmm_labels)) + geom_point() + theme_minimal())
```

```{r}
rm(wss, datExpr_k_means, h_clusts, cc_output, best_k, ICA_output, ICA_clusters_names, 
   signals_w_kurtosis, n_clust, gmm, gmm_points, gmm_labels, network, dend_meta, 
   best_power, c, viridis_age_cols, create_viridis_dict, pca)
```
<br><br>

## Compare clusterings

Using Adjusted Rand Index: 

* K-means, Hierarchical Clustering Consensus Clustering and Gaussian Mixtures seem to give very similar clusterings

* ASD seems to be captured best by K-Means and Consensus Clustering followed by Hierarchical clustering and Gaussian Mixtures

* No clusterings were able to capture any other phenotype feature

```{r}
clusters_plus_phenotype = clusterings
clusters_plus_phenotype[['Region']] = datMeta$Brain_lobe
clusters_plus_phenotype[['Sex']] = datMeta$Sex
clusters_plus_phenotype[['Age']] = datMeta$Age
clusters_plus_phenotype[['Subject']] = datMeta$Subject_ID
clusters_plus_phenotype[['ASD']] = datMeta$Diagnosis_

cluster_sim = data.frame(matrix(nrow = length(clusters_plus_phenotype), ncol = length(clusters_plus_phenotype)))
for(i in 1:(length(clusters_plus_phenotype))){
  cluster1 = as.factor(clusters_plus_phenotype[[i]])
  for(j in (i):length(clusters_plus_phenotype)){
    cluster2 = as.factor(clusters_plus_phenotype[[j]])
    cluster_sim[i,j] = adj.rand.index(cluster1, cluster2)
  }
}
colnames(cluster_sim) = names(clusters_plus_phenotype)
rownames(cluster_sim) = colnames(cluster_sim)

cluster_sim = cluster_sim %>% as.matrix %>% round(2)
heatmap.2(x = cluster_sim, Rowv = FALSE, Colv = FALSE, dendrogram = 'none', 
          cellnote = cluster_sim, notecol = 'black', trace = 'none', key = FALSE, 
          cexRow = 1, cexCol = 1, margins = c(7,7), colsep = 6, rowsep = 6, sepwidth = c(0.1,0.15),
          ColSideColors=c('#006699','#006699','#006699','#006699','#006699','#006699','#cc0066','#cc0066','#cc0066','#cc0066','#cc0066'))
 
rm(i, j, cluster1, cluster2, clusters_plus_phenotype, cluster_sim)
```
<br><br>

## Scatter plots
```{r, fig.width=10}
plot_points = datExpr_redDim %>% data.frame() %>% dplyr::select(PC1:PC3) %>%
              mutate(ID = rownames(.),                            Subject_ID = datMeta$Subject_ID,
                     KMeans = as.factor(clusterings[['KMeans']]),     Hierarchical = as.factor(clusterings[['HC']]),
                     Consensus = as.factor(clusterings[['CC']]),  ICA = as.factor(clusterings[['ICA']]),
                     GMM = as.factor(clusterings[['GMM']]),       WGCNA = as.factor(clusterings[['WGCNA']]),
                     Sex = as.factor(datMeta$Sex),                Region = as.factor(datMeta$Brain_lobe), 
                     Diagnosis = as.factor(datMeta$Diagnosis_),   Age = datMeta$Age)
```

Now, PC1 seems to separate samples by Diagnosis pretty well
```{r, warning=FALSE, fig.width=10, fig.height=8}
selectable_scatter_plot(plot_points, plot_points)
```
