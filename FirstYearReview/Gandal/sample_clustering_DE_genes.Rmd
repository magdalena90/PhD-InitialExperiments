---
title: 'Sample Clustering with DE genes'
output:
  html_document:
    code_folding: 'hide'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=TRUE, include=FALSE}
#setwd('/afs/inf.ed.ac.uk/user/s17/s1725186/Documents/PhD-InitialExperiments/FirstYearReview/R_markdowns')

library(tidyverse) ; library(reshape2) ; library(glue) ; library(plotly) ; library(plotlyutils)
library(RColorBrewer) ; library(viridis) ; require(gridExtra) ; library(GGally)
library(ConsensusClusterPlus)
library(JADE) ; library(MineICA) ; library(moments) ; library(fdrtool)
library(ClusterR)
library(WGCNA)
library(pdfCluster) ; library(gplots) ; library(dendextend)
```

Load preprocessed dataset (preprocessing code in data_preprocessing.Rmd)
```{r, echo=TRUE, include=FALSE, warning=FALSE, message=FALSE}
load('./../Data/Gandal/preprocessed_data.RData')
datExpr = datExpr %>% data.frame
DE_info = DE_info %>% data.frame
```

```{r}
glue('Number of genes: ', nrow(datExpr), '\n',
     'Number of samples: ', ncol(datExpr), ' (', sum(datMeta$Diagnosis_=='ASD'), ' ASD, ',
     sum(datMeta$Diagnosis_!='ASD'), ' controls)')
```

#### Filtering DE probes

Significance criteria: adjusted p-value<0.05 and abs(lfc)<log2(1.2) (absolute value because we care both for the under and overexpressed genes)
```{r}
datExpr = datExpr[DE_info$padj<0.05 & abs(DE_info$log2FoldChange)>log2(1.2),]
print(paste0(nrow(datExpr), ' probes left.'))
```

#### Dimensionality reduction using PCA

Since there are more genes than samples, we can perform PCA and reduce the dimension from 30K to 80 without losing any information
```{r}
pca = prcomp(t(datExpr))
datExpr_redDim = pca$x %>% data.frame
```

## Clustering Methods

```{r}
clusterings = list()
```
<br><br>

### K-means clustering

No recognisable best k, so chose k=6
```{r}
set.seed(123)
wss = sapply(1:15, function(k) kmeans(datExpr_redDim, k, nstart=25)$tot.withinss)
plot(wss, type='b', main='K-Means Clustering')
best_k = 6
abline(v = best_k, col='blue')

datExpr_k_means = kmeans(datExpr_redDim, best_k, nstart=25)
clusterings[['km']] = datExpr_k_means$cluster
```
<br><br>

### Hierarchical Clustering

Chose k=9 as best number of clusters.
```{r, fig.width=10}
h_clusts = datExpr_redDim %>% dist %>% hclust %>% as.dendrogram
h_clusts %>% plot
abline(h=39, col='blue')
best_k = 8
```

Clusters seem to be able to separate ASD and control samples pretty well and there are no noticeable patterns regarding sex, age or brain region in any cluster.

Colors:

- Diagnosis: Blue=control, Green=ASD

- Sex: Pink=Female, Blue=Male

- Brain region: Pink=Frontal, Green=Temporal, Blue=Parietal, Purple=Occipital

- Age: Purple=youngest, Yellow=oldest

```{r, fig.width=10}
clusterings[['hc']] = cutree(h_clusts, best_k)

create_viridis_dict = function(){
  min_age = datMeta$Age %>% min
  max_age = datMeta$Age %>% max
  viridis_age_cols = viridis(max_age - min_age + 1)
  names(viridis_age_cols) = seq(min_age, max_age)
  
  return(viridis_age_cols)
}
viridis_age_cols = create_viridis_dict()

dend_meta = datMeta[match(labels(h_clusts), rownames(datMeta)),] %>% 
            mutate('Diagnosis' = ifelse(Diagnosis_=='CTL','#008080','#86b300'), # Blue control, Green ASD
                   'Sex' = ifelse(Sex=='F','#ff6666','#008ae6'),                # Pink Female, Blue Male
                   'Region' = case_when(Brain_lobe=='Frontal'~'#F8766D',        # ggplot defaults for 4 colours
                                        Brain_lobe=='Temporal'~'#7CAE00',
                                        Brain_lobe=='Parietal'~'#00BFC4',
                                        Brain_lobe=='Occipital'~'#C77CFF'),
                   'Age' = viridis_age_cols[as.character(Age)]) %>%            # Purple: young, Yellow: old
            dplyr::select(Age, Region, Sex, Diagnosis)
h_clusts %>% set('labels', rep('', nrow(datMeta))) %>% set('branches_k_color', k=best_k) %>% plot
colored_bars(colors=dend_meta)
```
<br><br>

### Consensus Clustering

Chose the best clustering to be with k=5
```{r echo=FALSE, message=FALSE}
cc_output = datExpr_redDim %>% as.matrix %>% t %>% 
  ConsensusClusterPlus(maxK=10, reps=50, seed=123, title='./../Data/Gandal/consensusClustering/samples_DE_probes', plot='png')
best_k = 8
clusterings[['cc']] = cc_output[[best_k]]$consensusClass
```

```{r, echo=FALSE, out.width = '50%'}
knitr::include_graphics('./../Data/Gandal/consensusClustering/samples_DE_probes/consensus008.png')
```

*The rest of the output plots can be found in the Data/Gandal/consensusClustering/samples_DE_probes folder

<br><br>

### Independent Component Analysis

Following [this paper's](www.journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1002367) guidelines:

0. Run PCA and keep enough components to explain 60% of the variance (The 2 first components explain over 60% of the variance, so decided to keep the first 37 to accumulate 90% of the variance)

1. Run ICA with that same number of nbComp as principal components kept to then filter them

2. Select components with kurtosis > 3

3. Assign obs to genes with FDR<0.01 using the fdrtool package

```{r echo=FALSE, message=FALSE, warning=FALSE}
keep=37
ICA_output = datExpr_redDim[,1:keep] %>% runICA(nbComp=keep, method='JADE')
signals_w_kurtosis = ICA_output$S %>% data.frame %>% dplyr::select(names(apply(ICA_output$S, 2, kurtosis)>3))
ICA_clusters = apply(signals_w_kurtosis, 2, function(x) fdrtool(x, plot=F, verbose=F)$qval<0.01) %>% data.frame
ICA_clusters = ICA_clusters[colSums(ICA_clusters)>1]

clusterings[['ICA_min']] = rep(NA, nrow(ICA_clusters))
ICA_clusters_names = colnames(ICA_clusters)
for(c in ICA_clusters_names) clusterings[['ICA_min']][ICA_clusters[,c]] = c
```
<br>

It's not supposed to be a good method for clustering samples because ICA does not perform well with small samples (see [Figure 4](https://www.nature.com/articles/s41467-018-03424-4/figures/4) of [this paper](https://www.nature.com/articles/s41467-018-03424-4))

<!-- 2. Warnings: (Warning in fdrtool(x, plot = F): There may be too few input test statistics for reliable FDR calculations!) -->

Still, it leaves only 5 samples without a cluster
```{r, fig.width=10}
ICA_clusters %>% rowSums %>% table

ICA_clusters %>% mutate(cl_sum=rowSums(.)) %>% as.matrix %>% melt %>% ggplot(aes(Var2,Var1)) + 
  geom_tile(aes(fill=value)) + xlab('Clusters') + ylab('Samples') + 
  theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + coord_flip()
```
<br><br>

### WGCNA

This method doesn't work well:

Using power=5 as it is the first power that exceed the 0.85 threshold
```{r, warning=FALSE}
best_power = datExpr_redDim %>% t %>% pickSoftThreshold(powerVector = 1:30)
```

Running WGCNA with power=5
```{r}
# Best power
network = datExpr_redDim %>% t %>% blockwiseModules(power=best_power$powerEstimate, numericLabels=TRUE)
```

Cluster distribution
```{r}
table(network$colors)
clusterings[['WGCNA']] = network$colors
names(clusterings[['WGCNA']]) = rownames(datExpr_redDim)
```

Running WGCNA with power=10, leaves out even more genes and still only finds 1 cluster
```{r}
# 2nd best
network = datExpr_redDim %>% t %>% blockwiseModules(power=10, numericLabels=TRUE)
```

Cluster distribution
```{r}
table(network$colors)
```

Using the original expression dataset, the $R^2$ threshold is never achieved, getting closest at power 1, but still doesn't manage to find any clusters within the data
```{r, warning=FALSE}
best_power = datExpr %>% pickSoftThreshold(powerVector = 1:30)
```

Running WGCNA with power=1
```{r}
# Best power
network = datExpr %>% blockwiseModules(power=1, numericLabels=TRUE)
```

Cluster distribution
```{r}
table(network$colors)
```
<br><br>

### Gaussian Mixture Models with hard thresholding

Points don't seem to follow a Gaussian distribution no matter the number of clusters, chose 4 groups following the best k from K-means because the methods are similar
```{r}
n_clust = datExpr_redDim %>% Optimal_Clusters_GMM(max_clusters=50, criterion='BIC', plot_data=FALSE)
plot(n_clust, type='l', main='Bayesian Information Criterion to choose number of clusters')
best_k = 6 # copying k-means best_k
gmm = datExpr_redDim %>% GMM(best_k)
clusterings[['GMM']] = gmm$Log_likelihood %>% apply(1, which.max)
```

Plot of clusters with their centroids in gray
```{r}
gmm_points = rbind(datExpr_redDim, setNames(data.frame(gmm$centroids), names(datExpr_redDim)))
gmm_labels = c(clusterings[['GMM']], rep(NA, best_k)) %>% as.factor
ggplotly(gmm_points %>% ggplot(aes(x=PC1, y=PC2, color=gmm_labels)) + geom_point() + theme_minimal())
```

```{r}
rm(wss, datExpr_k_means, h_clusts, cc_output, best_k, ICA_output, ICA_clusters_names, 
   signals_w_kurtosis, n_clust, gmm, gmm_points, gmm_labels, network, dend_meta, 
   best_power, c, viridis_age_cols, create_viridis_dict, pca)
```
<br><br>

## Compare clusterings

Using Adjusted Rand Index: 

* K-means, Hierarchical Clustering Consensus Clustering and Gaussian Mixtures seem to give very similar clusterings (similar to the unfiltered version of this analysis)

* ASD seems to be captured best by K-Means and Consensus Clustering (although it's not that good)

* ICA seems to be the best clustering to capture Age (although it's not that good)

* No clusterings were able to capture any type of Sex or Brain Region structure

```{r}
clusters_plus_phenotype = clusterings
clusters_plus_phenotype[['Subject']] = datMeta$Subject_ID
clusters_plus_phenotype[['ASD']] = datMeta$Diagnosis_
clusters_plus_phenotype[['Region']] = datMeta$Brain_lobe
clusters_plus_phenotype[['Sex']] = datMeta$Sex
clusters_plus_phenotype[['Age']] = datMeta$Age

cluster_sim = data.frame(matrix(nrow = length(clusters_plus_phenotype), ncol = length(clusters_plus_phenotype)))
for(i in 1:(length(clusters_plus_phenotype))){
  cluster1 = as.factor(clusters_plus_phenotype[[i]])
  for(j in (i):length(clusters_plus_phenotype)){
    cluster2 = as.factor(clusters_plus_phenotype[[j]])
    cluster_sim[i,j] = adj.rand.index(cluster1, cluster2)
  }
}
colnames(cluster_sim) = names(clusters_plus_phenotype)
rownames(cluster_sim) = colnames(cluster_sim)

cluster_sim = cluster_sim %>% as.matrix %>% round(2)
heatmap.2(x = cluster_sim, Rowv = FALSE, Colv = FALSE, dendrogram = 'none', 
          cellnote = cluster_sim, notecol = 'black', trace = 'none', key = FALSE, 
          cexRow = 1, cexCol = 1, margins = c(7,7))
 
rm(i, j, cluster1, cluster2, clusters_plus_phenotype, cluster_sim)
```
<br><br>

## Scatter plots
```{r, fig.width=10}
plot_points = datExpr_redDim %>% data.frame() %>% dplyr::select(PC1:PC3) %>%
              mutate(ID = rownames(.),                            Subject_ID = datMeta$Subject_ID,
                     KMeans = as.factor(clusterings[['km']]),     Hierarchical = as.factor(clusterings[['hc']]),
                     Consensus = as.factor(clusterings[['cc']]),  ICA = as.factor(clusterings[['ICA_min']]),
                     GMM = as.factor(clusterings[['GMM']]),       WGCNA = as.factor(clusterings[['WGCNA']]),
                     Sex = as.factor(datMeta$Sex),                Region = as.factor(datMeta$Brain_lobe), 
                     Diagnosis = as.factor(datMeta$Diagnosis_),   Age = datMeta$Age)
```

Now, PC1 seems to separate samples by Diagnosis relatively well
```{r, warning=FALSE, fig.width=10, fig.height=8}
selectable_scatter_plot(plot_points, plot_points)
```
