---
title: 'WGCNA hclust distance metrics comparison'
output:
  html_document:
    code_folding: 'hide'
---
<br><br>

### Set pipeline:

- Using biweight correlation as correlation metric

- Elevating the correlation matrix to the best power for scale-free topology

- Using a Topological Overlap Matrix as distance matrix

- Using hierarchical clustering **comparing average linkage vs complete linkage**

- Extracting clusters using Dynamic Tree Cut from [Dynamic Tree Cut: in-depth description, tests and applications](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/BranchCutting/Supplement.pdf)

- Merging similar clusters using Module Eigengenes **comparing original clusters vs merged clusters vs two main clusters**

```{r, echo=TRUE, include=FALSE}
#setwd('/afs/inf.ed.ac.uk/user/s17/s1725186/Documents/PhD-InitialExperiments/FirstYearREview/WGCNABreakdown')

library(tidyverse) ; library(reshape2) ; library(glue) ; library(plotly) ; library(plotlyutils)
library(RColorBrewer) ; library(viridis) ; require(gridExtra) ; library(dendextend) ; library(gplots)
library(biomaRt) ; library(DESeq2) ; library(sva) ; library(WGCNA)
library(doParallel)
library(pdfCluster)
```

Load preprocessed dataset (preprocessing code in /../Gandal/data_preprocessing.Rmd)

```{r load_dataset, echo=TRUE, include=FALSE}
# Gandal dataset
load('./../Data/Gandal/preprocessed_data.RData')
datExpr = datExpr %>% data.frame
datGenes = datGenes %>% data.frame
DE_info = DE_info %>% data.frame

# GO Neuronal annotations
GO_annotations = read.csv('./../Data/GO_annotations/genes_GO_annotations.csv')
GO_neuronal = GO_annotations %>% filter(grepl('neuron', go_term)) %>% 
              mutate('ID'=as.character(ensembl_gene_id)) %>% 
              dplyr::select(-ensembl_gene_id) %>% distinct(ID) %>%
              mutate('Neuronal'=1)

clusterings = list()

rm(GO_annotations)
```

Keep DE genes
```{r}
datExpr = datExpr %>% filter(rownames(.) %in% rownames(DE_info)[DE_info$padj<0.05])
rownames(datExpr) = datGenes$feature_id[DE_info$padj<0.05 & !is.na(DE_info$padj)]
datGenes = datGenes %>% filter(feature_id %in% rownames(DE_info)[DE_info$padj<0.05])
DE_info = DE_info %>% filter(padj<0.05)

print(paste0('Keeping ', nrow(datExpr), ' genes and ', ncol(datExpr), ' samples'))
```
<br><br>

---

## Define a gene co-expression similarity

Using Biweight midcorrelation because it's more robust to outliers than regular correlation or Mutual Information score

```{r}
allowWGCNAThreads()

# MAD = median absolute deviation
cor_mat = datExpr %>% t %>% bicor
```


Correcting the correlation matrix from $s \in [-1,1]$ to $s \in [0,1]$. Two methods are proposed: $s_{ij}=|bw(i,j)|$ and $s_{ij}=\frac{1+bw(i,j)}{2}$

- Using $s_{ij}=\frac{1+bw(i,j)}{2}$, the strongest negative correlations (-1) get mapped to 0 (no correlation) and the zero correlated genes get mapped to the average correlation (0.5), which I don't think makes much sense
  
- Using $s_{ij}=|bw(i,j)|$ we lose the direction of the correlation, but at least we maintain the magnitude of the correlation of all the genes. Decided to use this one

```{r}
S = abs(cor_mat)
```
<br><br>

---

## Define a family of adjacency functions

- Sigmoid function: $a(i,j)=sigmoid(s_{ij}, \alpha, \tau_0) \equiv \frac{1}{1+e^{-\alpha(s_{ij}-\tau_0)}}$

- Power adjacency function: $a(i,j)=power(s_{ij}, \beta) \equiv |S_{ij}|^\beta$

Chose power adjacency function over the sigmoid function because it has only one parameter to adjust and both methods are supposed to lead to very similar results if the parameters are chosen with the scale-free topology criterion.

### Choosing a parameter value

Following the **scale-free topology criterion** because metabolic networks have been found to display approximate scale free topology

1. Only consider those parameter values that lead to a network satisfying scale-free topology at least approximately, e.g. signed $R^2 > 0.80$

```{r}
best_power = datExpr %>% t %>% pickSoftThreshold(powerVector = 1:15, RsquaredCut=0.8)

print(paste0('Best power for scale free topology: ', best_power$powerEstimate))
```

Elevate the matrix to the suggested power
```{r}
S_sft = S^best_power$powerEstimate
```
<br><br>


**Additional considerations**

2. The mean connectivity should be high so that the network contains enough information

What does high mean? using the power=5 we get a mean connectivity of 20, is this high?
```{r}
mean_connectivity = matrix(0, 15, 2) %>% data.frame
colnames(mean_connectivity) = c('power','mean_connectivity')
for(i in 1:15) mean_connectivity[i,] = c(i, mean(colSums(S^i)))

ggplotly(mean_connectivity %>% ggplot(aes(power, mean_connectivity)) + ylab('mean connectivity') +
         geom_vline(xintercept=best_power$powerEstimate, color='gray') + geom_point(color='#0099cc') + 
         scale_y_sqrt() + theme_minimal())

rm(mean_connectivity, best_power)
```

3. The slope $-\hat{\gamma}$ of the regression line between $log_{10}(p(k))$ and $log_{10}(k)$ should be around -1

Slope is 2.6 times as steep as it should be (maybe because the range of k is too narrow?)
```{r}
k_df = data.frame('connectivity'=colSums(S_sft), 'bucket'=cut(colSums(S_sft), 10)) %>% group_by(bucket) %>%
       dplyr::summarize(p_k=n(), k=mean(connectivity)) %>% mutate(p_k=p_k/sum(p_k))

lm(log10(p_k) ~ log10(k), data=k_df)

rm(k_df)
```


### Exploratory analysis of scale free topology adjacency matrix

- Degree distribution: the scale-free topology adjacency matrix does have an exponential behaviour in the degree distribution which wasn't there on the original matrix

**Note:** The slope is very steep, both axis are on sqrt scale
```{r, warning=FALSE, message=FALSE}
plot_data = data.frame('n'=1:nrow(S)^2, 'S'=sort(melt(S)$value), 'S_sft'=sort(melt(S_sft)$value))

plot_data %>% filter(n%%100==0) %>% dplyr::select(S, S_sft) %>% melt %>% 
              ggplot(aes(value, color=variable, fill=variable)) + geom_density(alpha=0.5) + 
              xlab('k') + ylab('p(k)') + scale_x_sqrt() + scale_y_sqrt() + theme_minimal()

rm(plot_data)
```

- "To visually inspect whether approximate scale-free topology is satisfied, one plots log 10 (p(k)) versus log 10 (k). A straight line is indicative of scale-free topology"

The example given in the article has a curve similar to this one and they say it's OK
```{r}
k_df = data.frame('connectivity'=colSums(S_sft), 'bucket'=cut(colSums(S_sft), 10)) %>% group_by(bucket) %>%
      dplyr::summarize(p_k=n(), k=mean(connectivity)) %>% mutate(p_k=p_k/sum(p_k))

# k_df %>% ggplot(aes(k,p_k)) + geom_point(color='#0099cc') + geom_smooth(method='lm', se=FALSE, color='gray') + 
#          ylab('p(k)') + scale_x_log10() + scale_y_log10() + theme_minimal()

k_df %>% ggplot(aes(log10(k),log10(p_k))) + geom_point(color='#0099cc') + geom_smooth(method='lm', se=FALSE, color='gray') + 
         ylab('p(k)') + theme_minimal()
```

- "To measure how well a network satisfies a scale-free topology, we propose to use the square of the correlation between log10(p(k)) and log10(k), i.e. the model fitting index $R^2$ of the linear model that regresses log10(p(k)) on log10(k)"

$R^2=0.83$ The $R^2$ we got from the pickSoftThreshold function
```{r}
lm(log10(p_k) ~ log10(k), data=k_df) %>% summary

rm(k_df)
```
<br><br>

---

## Defining a measure of node dissimilarity

Using topological overlap dissimilarity measure because it has been found to result in biologically meaningful modules

### Create Topological Overlap Matrix (TOM)

1st quartile is already 0.9852, most of the genes are very dissimilar
```{r}
TOM = S_sft %>% TOMsimilarity
dissTOM = 1-TOM

rownames(dissTOM) = rownames(S_sft)
colnames(dissTOM) = colnames(S_sft)

dissTOM %>% melt %>% summary
```
<br><br>

---

## Identifying gene modules

Using hierarchical clustering on the TOM-based dissimilarity matrix

### Using average linkage (following the paper)

In average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. 

It looks like instead of finding clusters, on each new separation this method discards outliers
```{r, fig.width=10}
dend = dissTOM %>% as.dist %>% hclust(method='average')
plot(dend, hang=0, labels=FALSE)
```

Instead of using a fixed height barch to cut the dendrogram into clusters, using a dynamic branch cutting approach taken from [Dynamic Tree Cut: in-depth description, tests and applications](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/BranchCutting/Supplement.pdf)

Two available methods:

1. **Dynamic Tree Cut:** top-down algorithm relying only on the dendrogram and respecting the order of the clustered objects on it. This method is less sensitive to parameter choice but also less flexible (I think I prefer robustness over flexibility, so I'm going to use this one)

2. Dynamic Hybrid Cut: builds the clusters from bottom up. In addition to information from the dendrogram, it utilizes dissimilarity information among the objects. Seems to me that relies on too many heuristics and has too many parameters to tune

Plus a post processing step:

- **deepSplit:** Controls the sensitivity of the algorithm to cluster splits. In Dynamic Tree it controls whether, after recursively processing all clusters, the algorithm should stop or whether it should re-process all clusters until there are no new clusters detected. I'll do the simple version first and don't re-process the clusters.

```{r}
modules = cutreeDynamicTree(dend, deepSplit=F, minModuleSize=20)
names(modules) = labels(dend)

clusterings[['avg_linkage']] = modules
```

#### Merging modules with similar expression profiles

"One can relate modules to each other by correlating the corresponding module eigengenes (Horvath et al., 2005). If two modules are highly correlated, one may want to merge them"

Calculate the "eigengenes" (1st principal component) of each module

```{r}
module_colors = c('gray', viridis(max(modules)))

MEs_output = datExpr %>% t %>% moduleEigengenes(colors=modules)
MEs = MEs_output$eigengenes
```

Merge similar modules
```{r, fig.width=10}
cor_dist = 1-cor(MEs)
dend_MEs = cor_dist %>% as.dist %>% hclust(method='average')
dend_MEs %>% as.dendrogram %>% plot(ylim=c(0, 0.6))
abline(h=0.55, col='#0099cc')
abline(h=0.228, col='#009999')

# Two main modules
tree_cut = cutree(dend_MEs, h=0.55)
top_modules = modules %>% replace(modules %in% (gsub('ME', '', names(tree_cut[tree_cut==1])) %>% as.numeric), 1) %>%
                          replace(modules %in% (gsub('ME', '', names(tree_cut[tree_cut==2])) %>% as.numeric), 2) %>%
                          replace(modules == 0, 0)
clusterings[['avg_linkage_top_clusters']] = top_modules


# Closest modules
tree_cut = cutree(dend_MEs, h=0.228)
merged_modules = modules
n=0
for(i in sort(unique(tree_cut))){
  n=n+1
  merged_modules = merged_modules %>% 
                   replace(modules %in% (gsub('ME', '', names(tree_cut[tree_cut==i])) %>% as.numeric), n)
}
merged_modules = merged_modules %>% replace(modules == 0, 0)

clusterings[['avg_linkage_merged']] = merged_modules
```

```{r, fig.width=10}
merged_module_colors = c('gray', viridis(length(unique(merged_modules))))
top_module_colors = c('gray', viridis(max(top_modules)))

dend_colors = data.frame('ID'=names(modules[labels(dend)]),
                         'OriginalModules' = module_colors[modules[dend$order]+1],
                         'MergedModules' = merged_module_colors[merged_modules[dend$order]+1],
                         'TopModules' = top_module_colors[top_modules[dend$order]+1])

dend %>% as.dendrogram(hang=0) %>% set('labels', rep('', nrow(dissTOM))) %>% plot(ylim=c(min(dend$height),1))
colored_bars(colors=dend_colors[,-1])

rm(MEs, dend, modules, module_colors, MEs_output, top_modules, merged_modules, tree_cut, 
   merged_module_colors, top_module_colors, dend_colors)
```
<br><br>


#### Using complete linkage

In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster.

```{r, fig.width=10}
dend = dissTOM %>% as.dist %>% hclust
plot(dend, hang=0, labels=FALSE)
```

```{r}
modules = cutreeDynamicTree(dend, deepSplit=F, minModuleSize=20)
names(modules) = labels(dend)

clusterings[['comp_linkage']] = modules
```

#### Merging modules with similar expression profiles

```{r, fig.width=10}
module_colors = c('gray', viridis(max(modules)))

MEs_output = datExpr %>% t %>% moduleEigengenes(colors=modules)
MEs = MEs_output$eigengenes

# Merge similar modules
cor_dist = 1-cor(MEs)
dend_MEs = cor_dist %>% as.dist %>% hclust(method='average')
dend_MEs %>% as.dendrogram %>% plot(ylim=c(0, 0.7))
abline(h=0.65, col='#0099cc')
abline(h=0.16, col='#009999')

# Two main modules
tree_cut = cutree(dend_MEs, h=0.65)
top_modules = modules %>% replace(modules %in% (gsub('ME', '', names(tree_cut[tree_cut==1])) %>% as.numeric), 1) %>%
                          replace(modules %in% (gsub('ME', '', names(tree_cut[tree_cut==2])) %>% as.numeric), 2) %>%
                          replace(modules == 0, 0)
clusterings[['comp_linkage_top_clusters']] = top_modules

# Closest modules
tree_cut = cutree(dend_MEs, h=0.16)
merged_modules = modules
n=0
for(i in sort(unique(tree_cut))){
  n=n+1
  merged_modules = merged_modules %>% 
                   replace(modules %in% (gsub('ME', '', names(tree_cut[tree_cut==i])) %>% as.numeric), n)
}
merged_modules = merged_modules %>% replace(modules == 0, 0)

clusterings[['comp_linkage_merged']] = merged_modules
```

```{r, fig.width=10}
merged_module_colors = c('gray', viridis(length(unique(merged_modules))))
top_module_colors = c('gray', viridis(max(top_modules)))

dend_colors = data.frame('ID'=names(modules[labels(dend)]),
                         'OriginalModules' = module_colors[modules[dend$order]+1],
                         'MergedModules' = merged_module_colors[merged_modules[dend$order]+1],
                         'TopModules' = top_module_colors[top_modules[dend$order]+1])

dend %>% as.dendrogram(hang=0) %>% set('labels', rep('', nrow(dissTOM))) %>% plot(ylim=c(min(dend$height),1))
colored_bars(colors=dend_colors[,-1])

rm(MEs, dend, modules, module_colors, MEs_output, top_modules, merged_modules, tree_cut, 
   merged_module_colors, top_module_colors, dend_colors)
```
<br><br>

---

### Exploratory analysis of clustering

#### Adjusted Rand Index comparison

```{r adj_rand_index, fig.width = 10}
cluster_sim = data.frame(matrix(nrow = length(clusterings), ncol = length(clusterings)))
for(i in 1:(length(clusterings))){
  cluster1 = sub(0, NA, clusterings[[i]]) %>% as.factor
  for(j in (i):length(clusterings)){
    cluster2 = sub(0, NA, clusterings[[j]]) %>% as.factor
    cluster_sim[i,j] = adj.rand.index(cluster1, cluster2)
  }
}
colnames(cluster_sim) = names(clusterings)
rownames(cluster_sim) = colnames(cluster_sim)

cluster_sim = cluster_sim %>% as.matrix %>% round(2)
heatmap.2(x = cluster_sim, Rowv = F, Colv = F, dendrogram = 'none', col=rev(brewer.pal(9,'YlOrRd'))[5:9],
          cellnote = cluster_sim, notecol = 'black', trace = 'none', key = FALSE, 
          cexRow = 1, cexCol = 1, margins = c(12,12))
 

rm(i, j, cluster1, cluster2, cluster_sim)
```

#### PCA

Cluster don't follow any strong patterns, at least in the first principal components
```{r pca, warning=FALSE, fig.width=10, fig.height=8}
pca = datExpr %>% prcomp

plot_data = data.frame('ID' = rownames(datExpr), 'PC1' = pca$x[,1], 'PC2' = pca$x[,2], 
                       'PC3' = pca$x[,3], 'PC4' = pca$x[,4], 'PC5' = pca$x[,5],
                       'avg_linkage' = sub(0,NA,clusterings[['avg_linkage']]) %>% as.factor, 
                       'avg_linkage_merged' = sub(0,NA,clusterings[['avg_linkage_merged']]) %>% as.factor, 
                       'avg_linkage_top_clusters' = sub(0,NA,clusterings[['avg_linkage_top_clusters']]) %>% as.factor, 
                       'comp_linkage' = sub(0,NA,clusterings[['comp_linkage']]) %>% as.factor, 
                       'comp_linkage_merged' = sub(0,NA,clusterings[['comp_linkage_merged']]) %>% as.factor, 
                       'comp_linkage_top_clusters' = sub(0,NA,clusterings[['comp_linkage_top_clusters']]) %>% as.factor)

selectable_scatter_plot(plot_data[,-1,], plot_data[,-1])

rm(pca, plot_data)
```
<br><br>
---

### Conclusions

- From the dendrograms, both clustering methods create well defined clusters

- Average linkage leaves less genes unclassified

- No method seems to have a strong relation with the first principal components

- Genes with high values for the 2nd PC seem to cluster together consistently (it's easiest to see this on the top clusters)

- The top modules seem to be related to the two clouds of points there are

- There doesn't seem to be a strong relation between clusterings

- **Average linkage seems to work better**

<br><br>

---

#### Save clusterings

```{r save_clusterings}

clusterings_file = './../Data/Gandal/clusterings.csv'

if(file.exists(clusterings_file)){

  df = read.csv(clusterings_file, row.names=1)
  
  if(!all(rownames(df) == rownames(datExpr))) stop('Gene ordering does not match the one in clusterings.csv!')
  
  for(clustering in names(clusterings)){
    df = df %>% mutate(!!clustering := sub(0, NA, clusterings[[clustering]]))
    rownames(df) = rownames(datExpr)
  }
  
} else {
  
  df = clusterings %>% unlist %>% matrix(nrow=length(clusterings), byrow=T) %>% t %>% data.frame %>% na_if(0)
  colnames(df) = names(clusterings)
  rownames(df) = rownames(datExpr)

}

write.csv(df, file=clusterings_file)

rm(clusterings_file, df, clustering)
```

---

#### Session info
```{r}
sessionInfo()
```
<br><br>

