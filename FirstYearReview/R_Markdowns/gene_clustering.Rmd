---
title: 'Probe Clustering'
output:
  html_document:
    code_folding: 'hide'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=TRUE, include=FALSE}
#setwd('/afs/inf.ed.ac.uk/user/s17/s1725186/Documents/PhD-InitialExperiments/FirstYearReview/R_markdowns')

library(tidyverse) ; library(reshape2) ; library(glue) ; library(plotly) ; library(plotlyutils)
library(RColorBrewer) ; library(viridis) ; require(gridExtra) ; library(GGally)
library(ConsensusClusterPlus)
library(JADE) ; library(MineICA) ; library(moments) ; library(fdrtool)
library(ClusterR)
library(WGCNA)
library(pdfCluster) ; library(gplots) ; library(dendextend)
```

Load preprocessed dataset (preprocessing code in data_preprocessing.Rmd)
```{r, echo=TRUE, include=FALSE, warning=FALSE, message=FALSE}
# Gandal dataset
load('./../Data/Gandal/preprocessed_data.RData')
datExpr = datExpr %>% data.frame
DE_info = DE_info %>% data.frame

# SFARI Genes
SFARI_genes = read_csv('./../Data/SFARI/SFARI_genes_with_ensembl_IDs.csv')

# GO Annotations
GO_annotations = read.csv('./../Data/GO_annotations/genes_GO_annotations.csv')
GO_neuronal = GO_annotations %>% filter(grepl('neuron', go_term)) %>% 
              mutate('ID' = as.character(ensembl_gene_id)) %>% 
              dplyr::select(-ensembl_gene_id) %>% distinct(ID) %>%
              mutate('Neuronal' = 1)

# Add SFARI scores and Neuronal functionality to DE_info
DE_info = DE_info %>% mutate('ID'=rownames(.)) %>% left_join(SFARI_genes, by='ID') %>% 
  mutate(`gene-score`=ifelse(is.na(`gene-score`), 'None', `gene-score`)) %>%
  distinct(ID, .keep_all = TRUE) %>% left_join(GO_neuronal, by='ID') %>%
  mutate(Neuronal=ifelse(is.na(Neuronal), 0, Neuronal)) %>%
  mutate(gene.score=ifelse(`gene-score`=='None' & Neuronal==1, 'Neuronal', `gene-score`))

rm(GO_annotations)
```

```{r}
glue('Number of genes: ', nrow(datExpr), '\n',
     'Number of samples: ', ncol(datExpr), ' (', sum(datMeta$Diagnosis_=='ASD'), ' ASD, ',
     sum(datMeta$Diagnosis_!='ASD'), ' controls)')
```

#### Increase filtering threshold

Clustering calculations are too heavy for 30K genes, increasing the filtering threshold to mean expression=6 to make calculations easier
```{r}
plot_data = data.frame('id'=rownames(datExpr), 'mean_expression' = rowMeans(datExpr), 'SFARI_score'=DE_info$`gene-score`!='None')
ggplotly(plot_data %>% ggplot(aes(x=mean_expression)) + geom_density(color='#0099cc', fill='#0099cc', alpha=0.3) + 
         geom_vline(xintercept=6, color='gray') + ggtitle('Probe Mean Expression distribution') + theme_minimal())
```

We keep most of the probes with a SFARI score
```{r}
ggplotly(plot_data %>% ggplot(aes(x=mean_expression, fill=SFARI_score, color=SFARI_score)) + geom_density(alpha=0.3) + 
         geom_vline(xintercept=5.9, color='gray') + ggtitle('Probe Mean Expression distribution') + theme_minimal())

print(paste0('Losing ', round(100*mean(plot_data$mean_expression[plot_data$SFARI_score==TRUE]<5.9),1),
             '% of the probes with SFARI score'))

datExpr = datExpr[plot_data$mean_expression>5.9,]
DE_info = DE_info[plot_data$mean_expression>5.9,]
datProbes = datProbes[plot_data$mean_expression>5.9,]

rm(plot_data)
```

#### Dimensionality reduction using PCA

To make calculations more efficient, we can perform PCA and keep the first 32 principal components which explain over 99.5% of the total variance
```{r}
pca = prcomp(datExpr)
datExpr_redDim = pca$x %>% data.frame %>% dplyr::select(PC1:PC32)
```

## Clustering Methods

```{r}
clusterings = list()

clusterings[['SFARI_score']] = DE_info$`gene-score`
names(clusterings[['SFARI_score']]) = rownames(DE_info)

clusterings[['SFARI_bool']] = DE_info$`gene-score`!='None'
names(clusterings[['SFARI_bool']]) = rownames(DE_info)

clusterings[['syndromic']] = DE_info$syndromic
names(clusterings[['syndromic']]) = rownames(DE_info)
```
<br><br>

### K-means clustering

```{r}
set.seed(123)
wss = sapply(1:10, function(k) kmeans(datExpr_redDim, k, iter.max=200, nstart=25,
                                      algorithm='MacQueen')$tot.withinss)
plot(wss, type='b', main='K-Means Clustering')
best_k = 4
abline(v = best_k, col='blue')

datExpr_k_means = kmeans(datExpr_redDim, best_k, iter.max=100, nstart=25)
clusterings[['km']] = datExpr_k_means$cluster
```
<br><br>

### Hierarchical Clustering

Chose k=11 as best number of clusters

```{r, warning=FALSE, fig.width=12, fig.height=6}
h_clusts = datExpr_redDim %>% dist %>% hclust
plot(h_clusts, hang = -1, cex = 0.6, labels=FALSE)
abline(h=28, col='blue')
best_k = 11
```

SFARI and Neuronal related probes seem to concentrate in the last clusters

```{r, warning=FALSE, fig.width=10, fig.height=6}
clusterings[['hc']] = cutree(h_clusts, best_k)

create_viridis_dict = function(){
  min_score = clusterings[['SFARI_score']] %>% as.numeric %>% min(na.rm=TRUE)
  max_score = clusterings[['SFARI_score']] %>% as.numeric %>% max(na.rm=TRUE)
  viridis_score_cols = viridis(max_score - min_score + 1)
  names(viridis_score_cols) = seq(min_score, max_score)
  
  return(viridis_score_cols)
}

viridis_score_cols = create_viridis_dict()

dend_meta = DE_info[match(labels(h_clusts), DE_info$ID),] %>% 
            mutate('SFARI_score' = viridis_score_cols[`gene-score`],                     # Purple: 2, Yellow: 6
                   'SFARI_bool' = ifelse(`gene-score` != 'None', '#21908CFF', 'white'),  # Acqua
                   'Syndromic' = ifelse(syndromic == T, 'orange', 'white'),
                   'Neuronal' = ifelse(ID %in% GO_neuronal$ID, '#666666','white')) %>% 
            dplyr::select(SFARI_score, SFARI_bool, Syndromic, Neuronal)

h_clusts %>% as.dendrogram %>% set('labels', rep('', nrow(datMeta))) %>% 
             set('branches_k_color', k=best_k) %>% plot
colored_bars(colors=dend_meta)
```

<br><br>

### Consensus Clustering

Samples are grouped into two big clusters, two small clusters and two outliers, the first big cluster has one main subcluster, two small subclusters and three outliers, and the second one has one main subcluster, one small one and three groups of outliers.

*Output plots in clustering_genes_04_12 folder
```{r echo=FALSE, message=FALSE}
# cc_output = datExpr_redDim %>% as.matrix %>% t %>% ConsensusClusterPlus(maxK=4, reps=5, seed=123,
#                                                    title='./../Data/Gandal/consensusClustering/probes/l1', plot='png')
# save(cc_output, file='./../Data/Gandal/consensusClustering/probes/l1/cc_output.RData')
load('./../Data/Gandal/consensusClustering/probes/l1/cc_output.RData')
best_k = 4 # 2 clusters and 2 outliers
clusterings[['cc_l1']] = cc_output[[best_k]]$consensusClass

clusterings[['cc_l2']] = clusterings[['cc_l1']]
# cc_output_c1 = datExpr_redDim %>% filter(clusterings[['cc_l1']]==1) %>% as.matrix %>% t %>%
#   ConsensusClusterPlus(maxK=8, reps=50, seed=123, title='./../Data/Gandal/consensusClustering/probes/l2_1/', plot='png')
# save(cc_output_c1, file='./../Data/Gandal/consensusClustering/probes/l2_1/cc_output.RData')
load('./../Data/Gandal/consensusClustering/probes/l2_1/cc_output.RData')
best_k = 1 # No subclusters
# clusterings[['cc_l2']][clusterings[['cc_l1']]==1] = cc_output_c1[[best_k]]$consensusClass %>%
#                                                     sapply(function(x) glue('1_', x))

# cc_output_c2 = datExpr_redDim %>% filter(clusterings[['cc_l1']]==2) %>% as.matrix %>% t %>%
#   ConsensusClusterPlus(maxK=8, reps=50, seed=123, title='./../Data/Gandal/consensusClustering/probes/l2_2/', plot='png')
# save(cc_output_c2, file='./../Data/Gandal/consensusClustering/probes/l2_2/cc_output.RData')
load('./../Data/Gandal/consensusClustering/probes/l2_2/cc_output.RData')
best_k = 1 # No subclusters
# clusterings[['cc_l2']][clusterings[['cc_l1']]==2] = cc_output_c2[[best_k]]$consensusClass %>%
#                                                     sapply(function(x) glue('2_', x))
```
<br><br>

### Independent Component Analysis

Following [this paper's](www.journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1002367) guidelines:

0. Run PCA and keep enough components to explain 60% of the variance (keeping 99% of the variance)

1. Run ICA with that same number of nbComp as principal components kept to then filter them

2. Select components with kurtosis > 3

3. Assign genes to clusters with FDR<0.01 using the fdrtool package

```{r echo=TRUE, results='hide', message=FALSE, warning=FALSE}
ICA_output = datExpr_redDim %>% runICA(nbComp=ncol(datExpr_redDim), method='JADE')
signals_w_kurtosis = ICA_output$S %>% data.frame %>% dplyr::select(names(apply(ICA_output$S, 2, kurtosis)>3))
ICA_clusters = apply(signals_w_kurtosis, 2, function(x) fdrtool(x, plot=F, verbose=F)$qval<0.01) %>% data.frame
ICA_clusters = ICA_clusters[colSums(ICA_clusters)>1]

clusterings[['ICA_min']] = rep(NA, nrow(ICA_clusters))
ICA_clusters_names = colnames(ICA_clusters)
for(c in ICA_clusters_names) clusterings[['ICA_min']][ICA_clusters[,c]] = c

clusterings[['ICA_NA']] = is.na(clusterings[['ICA_min']])
```

Leaves 73% all observations without cluster
```{r, fig.width=12}
ICA_clusters %>% rowSums %>% table

# ICA_clusters %>% mutate(cl_sum=rowSums(.)) %>% as.matrix %>% melt %>% ggplot(aes(Var2, Var1)) + 
#   geom_tile(aes(fill=value)) + xlab('Clusters') + ylab('Samples') + 
#   theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + coord_flip()
```

#### WGCNA

The soft R-squared value never reaches the threshold, but has its higher value at 30, so taking that power
```{r, warning=FALSE}
best_power = datExpr_redDim %>% t %>% pickSoftThreshold(powerVector = 1:30)
network = datExpr_redDim %>% t %>% blockwiseModules(power=30, numericLabels=TRUE)
clusterings[['WGCNA']] = network$colors
names(clusterings[['WGCNA']]) = rownames(datExpr_redDim)
```

It leaves 12% of the probes without a cluster and classifies almost all the rest into two big clusters
```{r}
clusterings[['WGCNA']] %>% table
```
<br><br>

### Gaussian Mixture Models with hard thresholding

The lowest BIC is achieved at 39 GM but the difference is not that big since 13, so choosing 13

```{r}
n_clust = datExpr_redDim %>% Optimal_Clusters_GMM(max_clusters=40, criterion='BIC', plot_data=FALSE)
plot(n_clust, type='l', main='Bayesian Information Criterion to choose number of clusters')
best_k = 13
gmm = datExpr_redDim %>% GMM(best_k)
clusterings[['GMM']] = gmm$Log_likelihood %>% apply(1, which.max)
```

Plot of clusters with their centroids in gray
```{r}
gmm_points = rbind(datExpr_redDim, setNames(data.frame(gmm$centroids), names(datExpr_redDim)))
gmm_labels = c(clusterings[['GMM']], rep(NA, best_k)) %>% as.factor
ggplotly(gmm_points %>% ggplot(aes(x=PC1, y=PC2, color=gmm_labels)) + geom_point() + theme_minimal())
```
<br><br>

### Manual separation

There are no recognisable groups in the PCA plot, but there seem to be three groups when plotting the density of the mean expression and none in the standard deviation
```{r, fig.width=10}
manual_clusters_data = data.frame('ID'=rownames(datExpr), 'Mean'=rowMeans(datExpr), 'SD'=apply(datExpr,1,sd))
p1 = manual_clusters_data %>% ggplot(aes(Mean)) + geom_density(color='#0099cc', fill='#0099cc', alpha=0.3) +
              scale_x_log10() + theme_minimal()
p2 = manual_clusters_data %>% ggplot(aes(SD)) + geom_density(color='#0099cc', fill='#0099cc', alpha=0.3) +
              scale_x_sqrt() + theme_minimal()
grid.arrange(p1, p2, ncol=2)
```

Separating probes into three Gaussians by their mean expression
```{r, fig.width=10}
gg_colour_hue = function(n) {
  hues = seq(15, 375, length=n+1)
  hcl(h = hues, l=65, c=100)[1:n]
}
n_clusters = 3
gmm_output = manual_clusters_data %>% dplyr::select(Mean) %>% GMM(n_clusters)
clusterings[['Manual_mean']] = gmm_output$Log_likelihood %>% apply(1, function(x) which.max(x))

plot_gaussians = manual_clusters_data %>% ggplot(aes(x=Mean)) +
  stat_function(fun=dnorm, n=100, colour=gg_colour_hue(n_clusters)[1], # 
                args=list(mean=gmm_output$centroids[1], sd=gmm_output$covariance_matrices[1])) +
  stat_function(fun=dnorm, n=100, colour=gg_colour_hue(n_clusters)[2], # 
                args=list(mean=gmm_output$centroids[2], sd=gmm_output$covariance_matrices[2])) +
  stat_function(fun=dnorm, n=100, colour=gg_colour_hue(n_clusters)[3], # 
                args=list(mean=gmm_output$centroids[3], sd=gmm_output$covariance_matrices[3])) +
  scale_x_log10() + theme_minimal()

plot_points = datExpr_redDim %>% ggplot(aes_string(x='PC1', y='PC2', color=as.factor(clusterings[['Manual_mean']]))) + 
          geom_point() + theme_minimal()

grid.arrange(plot_gaussians, plot_points, ncol=2)

clusterings[['Manual_mean']] %>% table
```

```{r, warning=FALSE}
# Clean up the environment a bit
rm(wss, datExpr_k_means, h_clusts, cc_output, cc_output_c1, cc_output_c2, best_k, ICA_output, 
   ICA_clusters_names, signals_w_kurtosis, n_clust, gmm, gmm_points, gmm_labels, network, plot_data,
   best_power, c, manual_clusters, manual_clusters_data, gmm_output, p1, p2, pca_data_projection, dend_meta, 
   plot_gaussians, plot_points, n_clusters, viridis_score_cols, gg_colour_hue, create_viridis_dict)
```

## Compare clusterings

Using Adjusted Rand Index:

* Clusterings are not very similar

* No clustering method resembles the SFARI scores at all

* K-means, Hierarchical clustering and Manual separation with Mean are the only ones that are similar between them

```{r, fig.width = 12}
cluster_sim = data.frame(matrix(nrow = length(clusterings), ncol = length(clusterings)))
for(i in 1:(length(clusterings))){
  cluster1 = as.factor(clusterings[[i]])
  for(j in (i):length(clusterings)){
    cluster2 = as.factor(clusterings[[j]])
    cluster_sim[i,j] = adj.rand.index(cluster1, cluster2)
  }
}
colnames(cluster_sim) = names(clusterings)
rownames(cluster_sim) = colnames(cluster_sim)

cluster_sim = cluster_sim %>% as.matrix %>% round(2)
heatmap.2(x = cluster_sim, Rowv = FALSE, Colv = FALSE, dendrogram = 'none', 
          cellnote = cluster_sim, notecol = 'black', trace = 'none', key = FALSE, 
          cexRow = 1, cexCol = 1, margins = c(7,7))
 

rm(i, j, cluster1, cluster2, cluster_sim)
```

### Scatter plots

* K-Means and Manual Mean only consider the first principal component for the separation and Hierarchical Cluster almost only

* ICA doesn't make any sense at all

* WGCNA leaves the points close to PC1=0 without classification

* SFARI genes seem to be everywhere (perhaps a bit more concentrated in high PC1 values)

* 1st PC seems to reflect the average level of expression of the genes
```{r}
plot_points = datExpr_redDim %>% data.frame() %>% dplyr::select(PC1:PC3) %>%
              mutate(ID = rownames(.),                                 k_means = as.factor(clusterings[['km']]),
                     hc = as.factor(clusterings[['hc']]),                   #cc_l1 = as.factor(clusterings[['cc_l1']]),
                     #cc_l2 = as.factor(clusterings[['cc_l2']]),             
                     ica = as.factor(clusterings[['ICA_min']]),
                     n_ica = log(rowSums(ICA_clusters)+1),                  gmm = as.factor(clusterings[['GMM']]),
                     wgcna = as.factor(clusterings[['WGCNA']]),
                     manual_mean = as.factor(clusterings[['Manual_mean']]), SFARI = as.factor(clusterings[['SFARI_score']]),
                     SFARI_bool = as.factor(clusterings[['SFARI_bool']]),   syndromic = as.factor(clusterings[['syndromic']])) %>%
              bind_cols(DE_info[DE_info$ID %in% rownames(datExpr_redDim),]) %>% 
              mutate(avg_expr = log2(rowMeans(datExpr)+1)[rownames(datExpr) %in% rownames(datExpr_redDim)])
rownames(plot_points) = plot_points$ID
```

```{r, warning=FALSE, fig.width = 12, fig.height=8}
selectable_scatter_plot(plot_points, plot_points)
```
