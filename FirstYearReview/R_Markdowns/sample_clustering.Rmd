---
title: 'Sample Clustering'
output:
  html_document:
    code_folding: 'hide'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=TRUE, include=FALSE}
#setwd('/afs/inf.ed.ac.uk/user/s17/s1725186/Documents/PhD-InitialExperiments/FirstYearReview/R_markdowns')

library(tidyverse) ; library(reshape2) ; library(glue) ; library(plotly) ; library(plotlyutils)
library(RColorBrewer) ; library(viridis) ; require(gridExtra) ; library(GGally)
library(ConsensusClusterPlus)
library(JADE) ; library(MineICA) ; library(moments) ; library(fdrtool)
library(ClusterR)
library(WGCNA)
library(pdfCluster) ; library(gplots) ; library(dendextend)
```

Load preprocessed dataset (preprocessing code in data_preprocessing.Rmd)
```{r, echo=TRUE, include=FALSE, warning=FALSE, message=FALSE}
load('./../Data/Gandal/preprocessed_data.RData')
datExpr = datExpr %>% data.frame
DE_info = DE_info %>% data.frame
```

```{r}
glue('Number of genes: ', nrow(datExpr), '\n',
     'Number of samples: ', ncol(datExpr), ' (', sum(datMeta$Diagnosis_=='ASD'), ' ASD, ',
     sum(datMeta$Diagnosis_!='ASD'), ' controls)')
```

#### Dimensionality reduction using PCA

Since there are more genes than samples, we can perform PCA and reduce the dimension from 30K to 80 without losing any information
```{r}
pca = prcomp(t(datExpr))
datExpr_redDim = pca$x %>% data.frame
```

## Clustering Methods

```{r}
clusterings = list()
```
<br><br>

### K-means clustering

No recognisable best k, so chose k=4
```{r}
set.seed(123)
wss = sapply(1:15, function(k) kmeans(datExpr_redDim, k, nstart=25)$tot.withinss)
plot(wss, type='b', main='K-Means Clustering')
best_k = 4
abline(v = best_k, col='blue')

datExpr_k_means = kmeans(datExpr_redDim, best_k, nstart=25)
clusterings[['km']] = datExpr_k_means$cluster
```
<br><br>

### Hierarchical Clustering

Chose k=9 as best number of clusters.
```{r, fig.width=10}
h_clusts = datExpr_redDim %>% dist %>% hclust %>% as.dendrogram
h_clusts %>% plot
abline(h=126, col='blue')
best_k = 9
```

Clusters seem to be able to separate ASD and control samples pretty well and there are no noticeable patterns regarding sex, age or brain region in any cluster.

Younger ASD samples seem to be more similar to control samples than older ASD ones. Perhaps there's a relation between age and severity of ASD because diagnosis are more inclusive nowadays, identifying mild autism cases that weren't recognised before, and most of these new diagnosis are made on kids?

Colors:

- Diagnosis: Blue=control, Green=ASD

- Sex: Pink=Female, Blue=Male

- Brain region: Pink=Frontal, Green=Temporal, Blue=Parietal, Purple=Occipital

- Age: Purple=youngest, Yellow=oldest

```{r, fig.width=10}
clusterings[['hc']] = cutree(h_clusts, best_k)

create_viridis_dict = function(){
  min_age = datMeta$Age %>% min
  max_age = datMeta$Age %>% max
  viridis_age_cols = viridis(max_age - min_age + 1)
  names(viridis_age_cols) = seq(min_age, max_age)
  
  return(viridis_age_cols)
}
viridis_age_cols = create_viridis_dict()

dend_meta = datMeta[match(labels(h_clusts), rownames(datMeta)),] %>% 
            mutate('Diagnosis' = ifelse(Diagnosis_=='CTL','#008080','#86b300'), # Blue control, Green ASD
                   'Sex' = ifelse(Sex=='F','#ff6666','#008ae6'),                # Pink Female, Blue Male
                   'Region' = case_when(Brain_lobe=='Frontal'~'#F8766D',        # ggplot defaults for 4 colours
                                        Brain_lobe=='Temporal'~'#7CAE00',
                                        Brain_lobe=='Parietal'~'#00BFC4',
                                        Brain_lobe=='Occipital'~'#C77CFF'),
                   'Age' = viridis_age_cols[as.character(Age)]) %>%            # Purple: young, Yellow: old
            dplyr::select(Age, Region, Sex, Diagnosis)
h_clusts %>% set('labels', rep('', nrow(datMeta))) %>% set('branches_k_color', k=best_k) %>% plot
colored_bars(colors=dend_meta)
```
<br><br>

### Consensus Clustering

Chose the best clustering to be with k=6
```{r echo=FALSE, message=FALSE}
cc_output = datExpr_redDim %>% as.matrix %>% t %>% 
  ConsensusClusterPlus(maxK=10, reps=50, seed=123, title='./../Data/Gandal/consensusClustering/samples', plot='png')
best_k = 6
clusterings[['cc']] = cc_output[[best_k]]$consensusClass
```

```{r, echo=FALSE, out.width = '50%'}
knitr::include_graphics('./../Data/Gandal/consensusClustering/samples/consensus006.png')
```

*The rest of the output plots can be found in the Data/Gandal/consensusClusterng/samples folder

<br><br>

### Independent Component Analysis

Following [this paper's](www.journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1002367) guidelines:

0. Run PCA and keep enough components to explain 60% of the variance (The 9 first components explain 60% of the variance, but decided to keep the first 26 to accumulate 80% of the variance)

1. Run ICA with that same number of nbComp as principal components kept to then filter them

2. Select components with kurtosis > 3

3. Assign obs to genes with FDR<0.01 using the fdrtool package

```{r echo=FALSE, message=FALSE, warning=FALSE}
keep=26
ICA_output = datExpr_redDim[,1:keep] %>% runICA(nbComp=keep, method='JADE')
signals_w_kurtosis = ICA_output$S %>% data.frame %>% dplyr::select(names(apply(ICA_output$S, 2, kurtosis)>3))
ICA_clusters = apply(signals_w_kurtosis, 2, function(x) fdrtool(x, plot=F, verbose=F)$qval<0.01) %>% data.frame
ICA_clusters = ICA_clusters[colSums(ICA_clusters)>1]

clusterings[['ICA_min']] = rep(NA, nrow(ICA_clusters))
ICA_clusters_names = colnames(ICA_clusters)
for(c in ICA_clusters_names) clusterings[['ICA_min']][ICA_clusters[,c]] = c
```
<br>

Not a good method for clustering samples because:

ICA does not perform well with small samples (see [Figure 4](https://www.nature.com/articles/s41467-018-03424-4/figures/4) of [this paper](https://www.nature.com/articles/s41467-018-03424-4))

<!-- 2. Warnings: (Warning in fdrtool(x, plot = F): There may be too few input test statistics for reliable FDR calculations!) -->

Still, it leaves only 14 samples without a cluster
```{r, fig.width=10}
ICA_clusters %>% rowSums %>% table

ICA_clusters %>% mutate(cl_sum=rowSums(.)) %>% as.matrix %>% melt %>% ggplot(aes(Var2,Var1)) + 
  geom_tile(aes(fill=value)) + xlab('Clusters') + ylab('Samples') + 
  theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + coord_flip()
```
<br><br>

### WGCNA

This method doesn't work:

Using the PCA reduced expression dataset give a very erratic $R^2$. Choosing the best power (7) as well as the second best (5) ends up leaving all genes without cluster
```{r, warning=FALSE}
best_power = datExpr_redDim %>% t %>% pickSoftThreshold(powerVector = 1:20)
```

Running WGCNA with power=7
```{r}
# Best power
network = datExpr_redDim %>% t %>% blockwiseModules(power=best_power$powerEstimate, numericLabels=TRUE)
```

Cluster distribution
```{r}
table(network$colors)
```

Running WGCNA with power=5
```{r}
# 2nd best
network = datExpr_redDim %>% t %>% blockwiseModules(power=5, numericLabels=TRUE)
```

Cluster distribution
```{r}
table(network$colors)
```

Using the original expression dataset, the $R^2$ threshold is never achieved, altohugh it gets closest at 11, but still doesn't manage to find any clusters within the data
```{r, warning=FALSE}
best_power = datExpr %>% pickSoftThreshold(powerVector = 1:30)
```

Running WGCNA with power=11
```{r}
# Best power
network = datExpr %>% blockwiseModules(power=11, numericLabels=TRUE)
```

Cluster distribution
```{r}
table(network$colors)
```
<br><br>

### Gaussian Mixture Models with hard thresholding

Points don't seem to follow a Gaussian distribution no matter the number of clusters, chose 4 groups following the best k from K-means because the methods are similar
```{r}
n_clust = datExpr_redDim %>% Optimal_Clusters_GMM(max_clusters=50, criterion='BIC', plot_data=FALSE)
plot(n_clust, type='l', main='Bayesian Information Criterion to choose number of clusters')
best_k = 4 # copying k-means best_k
gmm = datExpr_redDim %>% GMM(best_k)
clusterings[['GMM']] = gmm$Log_likelihood %>% apply(1, which.max)
```

Plot of clusters with their centroids in gray
```{r}
gmm_points = rbind(datExpr_redDim, setNames(data.frame(gmm$centroids), names(datExpr_redDim)))
gmm_labels = c(clusterings[['GMM']], rep(NA, best_k)) %>% as.factor
ggplotly(gmm_points %>% ggplot(aes(x=PC1, y=PC2, color=gmm_labels)) + geom_point() + theme_minimal())
```

```{r}
rm(wss, datExpr_k_means, h_clusts, cc_output, best_k, ICA_output, ICA_clusters_names, 
   signals_w_kurtosis, n_clust, gmm, gmm_points, gmm_labels, network, dend_meta, 
   best_power, c, viridis_age_cols, create_viridis_dict)
```
<br><br>

## Compare clusterings

Using Adjusted Rand Index: 

* K-means, Hierarchical Clustering and Consensus Clustering and Gaussian Mixtures seem to give relatively similar clusterings

* K-means and Gaussian Mixtures are the most similar clusterings, which makes sense because the two methods are similar

* ASD seems to be captured best by Gaussian Mixtures (although it's not that good)

* ICA seems to be the best clustering to capture Age (although it's not that good)

* No clusterings were able to capture any type of Sex or Brain Region structure

```{r}
clusters_plus_phenotype = clusterings
clusters_plus_phenotype[['Subject']] = datMeta$Subject_ID
clusters_plus_phenotype[['ASD']] = datMeta$Diagnosis_
clusters_plus_phenotype[['Region']] = datMeta$Brain_lobe
clusters_plus_phenotype[['Sex']] = datMeta$Sex
clusters_plus_phenotype[['Age']] = datMeta$Age

cluster_sim = data.frame(matrix(nrow = length(clusters_plus_phenotype), ncol = length(clusters_plus_phenotype)))
for(i in 1:(length(clusters_plus_phenotype))){
  cluster1 = as.factor(clusters_plus_phenotype[[i]])
  for(j in (i):length(clusters_plus_phenotype)){
    cluster2 = as.factor(clusters_plus_phenotype[[j]])
    cluster_sim[i,j] = adj.rand.index(cluster1, cluster2)
  }
}
colnames(cluster_sim) = names(clusters_plus_phenotype)
rownames(cluster_sim) = colnames(cluster_sim)

cluster_sim = cluster_sim %>% as.matrix %>% round(2)
heatmap.2(x = cluster_sim, Rowv = FALSE, Colv = FALSE, dendrogram = 'none', 
          cellnote = cluster_sim, notecol = 'black', trace = 'none', key = FALSE, 
          cexRow = 1, cexCol = 1, margins = c(7,7))
 
rm(i, j, cluster1, cluster2, clusters_plus_phenotype, cluster_sim)
```
<br><br>

## Scatter plots
```{r, fig.width=10}
plot_points = datExpr_redDim %>% data.frame() %>% dplyr::select(PC1:PC3) %>%
              mutate(ID = rownames(.),                            Subject_ID = datMeta$Subject_ID,
                     KMeans = as.factor(clusterings[['km']]),     Hierarchical = as.factor(clusterings[['hc']]),
                     Consensus = as.factor(clusterings[['cc']]),  ICA = as.factor(clusterings[['ICA_min']]),
                     GMM = as.factor(clusterings[['GMM']]),
                     Sex = as.factor(datMeta$Sex),                Region = as.factor(datMeta$Brain_lobe), 
                     Diagnosis = as.factor(datMeta$Diagnosis_),   Age = datMeta$Age)
```

```{r, warning=FALSE, fig.width=10, fig.height=8}
selectable_scatter_plot(plot_points, plot_points)
```
